@article{Demsar2016BalancedMixture,
    title = {{A Balanced Mixture of Antagonistic Pressures Promotes the Evolution of Parallel Movement}},
    year = {2016},
    journal = {Scientific Reports},
    author = {Dem{\v{s}}ar, Jure and {\v{S}}trumbelj, Erik and Lebar Bajec, Iztok},
    volume = {6},
    doi = {10.1038/srep39428}
}

@article{Demsar2017LinguisticEvolution,
    title = {{Evolution of Collective Behaviour in an Artificial World Using Linguistic Fuzzy Rule-Based Systems}},
    year = {2017},
    journal = {PLoS ONE},
    author = {Dem{\v{s}}ar, Jure and Lebar Bajec, Iztok},
    number = {1},
    pages = {1--20},
    volume = {12},
    doi = {10.1371/journal.pone.0168876}
}

@misc{gpl2,
author = {},
title = {Domain Adaptation with Generative Pseudo-Labeling (GPL) | Pinecone},
howpublished = {\url{https://www.pinecone.io/learn/series/nlp/gpl/}},
month = {},
year = {},
note = {(Accessed on 03/21/2024)}
}

@inproceedings{gpl,
    title = "{GPL}: Generative Pseudo Labeling for Unsupervised Domain Adaptation of Dense Retrieval",
    author = "Wang, Kexin  and
      Thakur, Nandan  and
      Reimers, Nils  and
      Gurevych, Iryna",
    editor = "Carpuat, Marine  and
      de Marneffe, Marie-Catherine  and
      Meza Ruiz, Ivan Vladimir",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jul,
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.168",
    doi = "10.18653/v1/2022.naacl-main.168",
    pages = "2345--2360"
}

@article{reimers2019sentence,
  title={Sentence-bert: Sentence embeddings using siamese bert-networks},
  author={Reimers, Nils and Gurevych, Iryna},
  journal={arXiv preprint arXiv:1908.10084},
  year={2019}
}

@inproceedings{tsdae,
    title = "{TSDAE}: Using Transformer-based Sequential Denoising Auto-Encoderfor Unsupervised Sentence Embedding Learning",
    author = "Wang, Kexin  and
      Reimers, Nils  and
      Gurevych, Iryna",
    editor = "Moens, Marie-Francine  and
      Huang, Xuanjing  and
      Specia, Lucia  and
      Yih, Scott Wen-tau",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2021",
    month = nov,
    year = "2021",
    address = "Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.findings-emnlp.59",
    doi = "10.18653/v1/2021.findings-emnlp.59",
    pages = "671--688",
    abstract = "Learning sentence embeddings often requires a large amount of labeled data. However, for most tasks and domains, labeled data is seldom available and creating it is expensive. In this work, we present a new state-of-the-art unsupervised method based on pre-trained Transformers and Sequential Denoising Auto-Encoder (TSDAE) which outperforms previous approaches by up to 6.4 points. It can achieve up to 93.1{\%} of the performance of in-domain supervised approaches. Further, we show that TSDAE is a strong domain adaptation and pre-training method for sentence embeddings, significantly outperforming other approaches like Masked Language Model. A crucial shortcoming of previous studies is the narrow evaluation: Most work mainly evaluates on the single task of Semantic Textual Similarity (STS), which does not require any domain knowledge. It is unclear if these proposed methods generalize to other domains and tasks. We fill this gap and evaluate TSDAE and other recent approaches on four different datasets from heterogeneous domains.",
}

@misc{pinecone_2021,
    author = {},
    title = {Unsupervised Training for Sentence Transformers},
    url = {https://www.pinecone.io/learn/series/nlp/unsupervised-training-sentence-transformers/},
    journal = {Pinecone.io},
    year = {2021},
    note = {(Accessed on 03/21/2024)}
}


@InProceedings{unsupervised-domain-adaptation,
  title = 	 {Unsupervised Domain Adaptation by Backpropagation},
  author = 	 {Ganin, Yaroslav and Lempitsky, Victor},
  booktitle = 	 {Proceedings of the 32nd International Conference on Machine Learning},
  pages = 	 {1180--1189},
  year = 	 {2015},
  editor = 	 {Bach, Francis and Blei, David},
  volume = 	 {37},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {Lille, France},
  month = 	 {07--09 Jul},
  publisher =    {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v37/ganin15.pdf},
  url = 	 {https://proceedings.mlr.press/v37/ganin15.html},
  abstract = 	 {Top-performing deep architectures are trained on massive amounts of labeled data. In the absence of labeled data for a certain task, domain adaptation often provides an attractive option given that labeled data of similar nature but from a different domain (e.g. synthetic images) are available. Here, we propose a new approach to domain adaptation in deep architectures that can be trained on large amount of labeled data from the source domain and large amount of unlabeled data from the target domain (no labeled target-domain data is necessary). As the training progresses, the approach promotes the emergence of "deep" features that are (i) discriminative for the main learning task on the source domain and (ii) invariant with respect to the shift between the domains. We show that this adaptation behaviour can be achieved in almost any feed-forward model by augmenting it with few standard layers and a simple new gradient reversal layer. The resulting augmented architecture can be trained using standard backpropagation. Overall, the approach can be implemented with little effort using any of the deep-learning packages. The method performs very well in a series of image classification experiments, achieving adaptation effect in the presence of big domain shifts and outperforming previous state-of-the-art on Office datasets.}
}

@article{knowlage-distillation,
title = {Knowledge distillation methods for efficient unsupervised adaptation across multiple domains},
journal = {Image and Vision Computing},
volume = {108},
pages = {104096},
year = {2021},
issn = {0262-8856},
doi = {https://doi.org/10.1016/j.imavis.2021.104096},
url = {https://www.sciencedirect.com/science/article/pii/S0262885621000019},
author = {Le Thanh Nguyen-Meidine and Atif Belal and Madhu Kiran and Jose Dolz and Louis-Antoine Blais-Morin and Eric Granger},
keywords = {Deep learning, Convolutional NNs, Knowledge distillation, Unsupervised domain adaptation, CNN acceleration and compression},
abstract = {Beyond the complexity of CNNs that require training on large annotated datasets, the domain shift between design and operational data has limited the adoption of CNNs in many real-world applications. For instance, in person re-identification, videos are captured over a distributed set of cameras with non-overlapping viewpoints. The shift between the source (e.g. lab setting) and target (e.g. cameras) domains may lead to a significant decline in recognition accuracy. Additionally, state-of-the-art CNNs may not be suitable for such real-time applications given their computational requirements. Although several techniques have recently been proposed to address domain shift problems through unsupervised domain adaptation (UDA), or to accelerate/compress CNNs through knowledge distillation (KD), we seek to simultaneously adapt and compress CNNs to generalize well across multiple target domains. In this paper, we propose a progressive KD approach for unsupervised single-target DA (STDA) and multi-target DA (MTDA) of CNNs. Our method for KD-STDA adapts a CNN to a single target domain by distilling from a larger teacher CNN, trained on both target and source domain data in order to maintain its consistency with a common representation. This method is extended to address MTDA problems, where multiple teachers are used to distill multiple target domain knowledge to a common student CNN. A different target domain is assigned to each teacher model for UDA, and they alternatively distill their knowledge to the student model to preserve specificity of each target, instead of directly combining the knowledge from each teacher using fusion methods. Our proposed approach is compared against state-of-the-art methods for compression and STDA of CNNs on the Office31 and ImageClef-DA image classification datasets. It is also compared against state-of-the-art methods for MTDA on Digits, Office31, and OfficeHome. In both settings – KD-STDA and KD-MTDA – results indicate that our approach can achieve the highest level of accuracy across target domains, while requiring a comparable or lower CNN complexity.}
}

@Inbook{transfer-leraning-domain-adaptation,
author="Kamath, Uday
and Liu, John
and Whitaker, James",
title="Transfer Learning: Domain Adaptation",
bookTitle="Deep Learning for NLP and Speech Recognition ",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="495--535",
abstract="Domain adaptation is a form of transfer learning, in which the task remains the same, but there is a domain shift or a distribution change between the source and the target. As an example, consider a model that has learned to classify reviews on electronic products for positive and negative sentiments, and is used for classifying the reviews for hotel rooms or movies. The task of sentiment analysis remains the same, but the domain (electronics and hotel rooms) has changed. The application of the model to a separate domain poses many problems because of the change between the training data and the unseen testing data, typically known as domain shift. For example, sentences containing phrases such as ``loud and clear'' will be mostly considered positive in electronics whereas negative in hotel room reviews. Similarly, usage of keywords such as ``lengthy'' or ``boring'' which may be prevalent in domains such as book reviews might be completely absent in domains such as kitchen equipment reviews.",
isbn="978-3-030-14596-5",
doi="10.1007/978-3-030-14596-5_11",
url="https://doi.org/10.1007/978-3-030-14596-5_11"
}

 @misc{SentiNews,
 title = {Manually sentiment annotated Slovenian news corpus {SentiNews} 1.0},
 author = {Bu{\v c}ar, Jo{\v z}e},
 url = {http://hdl.handle.net/11356/1110},
 note = {Slovenian language resource repository {CLARIN}.{SI}},
 copyright = {Creative Commons - Attribution-{ShareAlike} 4.0 International ({CC} {BY}-{SA} 4.0)},
 issn = {2820-4042},
 year = {2017} }
‌
 @misc{siParl,
 title = {Slovenian parliamentary corpus (1990-2022) {siParl} 3.0},
 author = {Pan{\v c}ur, Andrej and Erjavec, Toma{\v z} and Meden, Katja and Ojster{\v s}ek, Mihael and {\v S}orn, Mojca and Blaj Hribar, Neja},
 url = {http://hdl.handle.net/11356/1748},
 note = {Slovenian language resource repository {CLARIN}.{SI}},
 copyright = {Creative Commons - Attribution-{ShareAlike} 4.0 International ({CC} {BY}-{SA} 4.0)},
 issn = {2820-4042},
 year = {2022} }

