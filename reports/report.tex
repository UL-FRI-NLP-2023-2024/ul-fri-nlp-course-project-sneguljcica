%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% FRI Data Science_report LaTeX Template
% Version 1.0 (28/1/2020)
% 
% Jure Demšar (jure.demsar@fri.uni-lj.si)
%
% Based on MicromouseSymp article template by:
% Mathias Legrand (legrand.mathias@gmail.com) 
% With extensive modifications by:
% Antonio Valente (antonio.luis.valente@gmail.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------
\documentclass[fleqn,moreauthors,10pt]{ds_report}
\usepackage[english]{babel}

\graphicspath{{fig/}}




%----------------------------------------------------------------------------------------
%	ARTICLE INFORMATION
%----------------------------------------------------------------------------------------

% Header
\JournalInfo{FRI Natural language processing course 2021}

% Interim or final report
\Archive{Project report} 
%\Archive{Final report} 

% Article title
\PaperTitle{Unsupervised Domain adaptation for Sentence Classification} 

% Authors (student competitors) and their info
\Authors{Marko Možina, Peter Kosem, Aljaž Konec}

% Advisors
\affiliation{\textit{Advisors: Boshko Koloski}}

% Keywords
\Keywords{Unsupervised Sentence Classification, Generative Pseudo Labeling, Transformer-based Denoising AutoEncoder}
\newcommand{\keywordname}{Keywords}


%----------------------------------------------------------------------------------------
%	ABSTRACT
%----------------------------------------------------------------------------------------

\Abstract{
    TBA
}

%----------------------------------------------------------------------------------------

\begin{document}

% Makes all text pages the same height
\flushbottom 

% Print the title and abstract box
\maketitle 

% Removes page numbering from the first page
\thispagestyle{empty} 

%----------------------------------------------------------------------------------------
%	ARTICLE CONTENTS
%----------------------------------------------------------------------------------------

\section*{Introduction}

Natural Language Processing (NLP) is most commonly used applications like sentiment analysis, spam detection and topic categorization. 
Applying general NLP models to specialized domains, where unique terminologies and contexts are needed, can affect model performance. 
Sentence-transformer models, while effective for generating sentence embeddings, often fall short in these specialized settings without domain-specific tuning.

This study aims to tackle this by enhancing sentence representation in specialized domains through unsupervised domain adaptation techniques \cite{unsupervised-domain-adaptation, transfer-leraning-domain-adaptation, knowlage-distillation}, specifically Transformer-based Denoising AutoEncoder (TSDAE) \cite{tsdae} and Generative Pse\-udo Labeling (GPL) \cite{gpl}. 
These methods intend to refine the embedding space, making models more sensitive and accurate for specific domains, thereby improving sentence classification outcomes.

The project focuses on Slovenian language and as such will use Slovenian specialized domains SentiNews \cite{SentiNews} and siParl \cite{siParl} for evaluating the adaptation to a specialized domain.


%------------------------------------------------

\section*{Methods}

This section outlines the approach taken to adapt sentence-transformer models for improved sentence classification within specialized domains, leveraging the techniques of Transformer-based Denoising AutoEncoder (TSDAE) and Generative Pse\-udo Labeling (GPL).

\subsection*{Generative pseudo labeling (GPL)}

The ability to effectively process and classify text across diverse domains remains a challenge in natural language processing. 
Traditional models often fail when applied outside their training domain due to the unique linguistic characteristics of the domain.
For instance, the world 'loud' has a positive sentiment if used to describe a speaker and a negative sentiment if used in a review of a hotel.
This highlights the need for domain adaptation techniques capable of leveraging unlabeled textual data that is prevalent in specialized fields. 
Generative Pseudo Labeling (GPL) \cite{gpl} offers a novel approach to utilize unlabeled data for enhancing model adaptability and performance in specialized, often smaller, domains.

Generative Pseudo Labeling (GPL) is predicated on the innovative use of unlabeled data to improve model functionality in target domains. 
The GPL methodology unfolds in two stages:
\begin{enumerate}
    \item \textbf{Pseudo Label Generation:} A pre-trained model, proficient in a related but distinct task, assigns provisional labels to unlabeled target domain data. These initial labels, derived from the model's pre-existing knowledge, serve as a foundational step for domain adaptation \cite{reimers2019sentence}.
    
    \item \textbf{Refinement through Generative Modeling:} Subsequently, the model undergoes a self-enhancement phase, refining its capabilities by learning from the data directly. 
    This involves generative models that discern and adapt to the underlying patterns specific to the target domain, thereby aligning the model more closely with the target domain's characteristics.
\end{enumerate}

Our project seeks to leverage GPL for the unsupervised domain adaptation of sentence-transformer models, aiming to bolster sentence classification accuracy within specialized domains. The application process is outlined as follows:
\begin{enumerate}
    \item \textbf{Initial Model Training:} Employing a pre-trained sent\-ence-transformer model, leveraging its extensive knowledge base for a preliminary understanding of the target domain \cite{reimers2019sentence}.
    
    \item \textbf{Pseudo Label Creation:} Generating pseudo labels for the Slovenian classification dataset (e.g., SentiNews) with the pre-trained model, bridging the model's knowledge from general to specific domains.
    
    \item \textbf{Model Adaptation via GPL:} A generative model refines the sentence embeddings and classification efficacy of the sentence-transformer, emphasizing the adaptation to capture domain-specific nuances accurately.
    
    \item \textbf{Iterative Refinement and Evaluation:} Through continuous refinement and evaluation, the model's performance is iteratively improved, ensuring its alignment with the project's goals.
\end{enumerate}



\subsection*{Transformer-based Denoising AutoEncoder (TSDAE)}

The core idea of TSDAE \cite{tsdae} is to introduce noise to input sequences by deleting or swapping tokens (e.g., words). 
This corrupted input is then fed into the encoder component of the TSDAE, which consists of transformer layers that encode the corrupted input data into a latent space representation of sentence vectors. 
The decoder network, then aims to reconstruct the original input data from the latent representation.
Below is a brief explanation of the sequential process of TSDAE:

\begin{enumerate}
    \item \textbf{Corruption:} The input data is corrupted with noise, introducing variations and disturbances into the data. Adopting only deletion as the input noise and setting the deletion ratio to 0.6 performs best per~\cite{tsdae}.
    \item \textbf{Encoding:} The corrupted input data is fed into the encoder, which consists of transformer layers. These layers transform the input data into a latent space representation called sentence vector, capturing essential features while filtering out noise.
    \item \textbf{Decoding:} The latent representation obtained from the encoder is passed through the decoder, also composed of Transformer layers. The decoder aims to reconstruct the original, clean input data from the latent representation.
    \item \textbf{Reconstruction:} The classifier token (CSL) embedding is used during reconstruction from token-level to sentence-level representation~\cite{pinecone_2021}.
    \item \textbf{Training:} The TSDAE optimizes its parameters by minimizing the reconstruction error between the denoised output generated by the decoder and the original, clean input data. This process occurs iteratively, allowing the model to learn effective denoising strategies.
\end{enumerate}

For fine-tuning the model, we need to set up the training data (which is nothing more than text data, since the model is unsupervised), a pretrained model prepared for producing sentence vectors and a loss function.
By leveraging the Transformer architecture, TSDAEs can efficiently capture complex dependencies and patterns in the data, making them effective for denoising tasks across various domains, including natural language processing. 
Despite its inability to match the performance of supervised methods, TSDAE remains valuable, particularly in scenarios where data is unlabeled or difficult to obtain.

\subsection*{Specialized Domains}
As mentioned before, we primarly focus on two specialized domains in Slovenian language: SentiNews \cite{SentiNews} and siParl \cite{siParl}.
SentiNews is a Slovenian news dataset containing around 10 427 articles form different news sources.
These articles is labeled with sentiment labels using the five-level Lickert scale on three levels of granularity (document, paragraph and sentence).

SiParl is a Slovenian parliamentary dataset containing transcripts of parliamentary sessions.
The whole dataset is arounf 11 thousand session, containing 1 million speaches and 200 million words.
This dataset is not labeled and we will use it for unsupervised domain adaptation.



%----------------------------------------------------------------------------------------
%	REFERENCE LIST
%----------------------------------------------------------------------------------------
\bibliographystyle{acm}
\bibliography{report}


\end{document}